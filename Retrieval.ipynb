{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval exercise\n",
    "\n",
    "In this exercise, you will implement the query likelihood model with Jelinek-Mercer smoothing. This assignment builds on the previous assignment and you will need to reuse some code from the Indexing notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build the index\n",
    "Download the MS MARCO passage collection and build an index using [Pyserini](https://github.com/castorini/pyserini).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =======Your code=======\n",
    "\n",
    "# =======================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download and read the query file\n",
    "You will rank MSMARCO passages for this set of queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-09-15 22:10:45--  http://gem.cs.ru.nl/IR-Course-2021-2022/queries.txt\n",
      "Resolving gem.cs.ru.nl (gem.cs.ru.nl)... 131.174.31.31\n",
      "Connecting to gem.cs.ru.nl (gem.cs.ru.nl)|131.174.31.31|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2275 (2,2K) [text/plain]\n",
      "Saving to: ‘queries.txt.2’\n",
      "\n",
      "queries.txt.2       100%[===================>]   2,22K  --.-KB/s    in 0,001s  \n",
      "\n",
      "2021-09-15 22:10:45 (2,73 MB/s) - ‘queries.txt.2’ saved [2275/2275]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://gem.cs.ru.nl/IR-Course-2021-2022/queries.txt\n",
    "    \n",
    "queries = dict()\n",
    "with open(\"queries.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        cols = line.split(\"\\t\")\n",
    "        queries[cols[0].strip()] = cols[1].strip()\n",
    "\n",
    "# queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement the retrieval model\n",
    "You will implement language model with Jelinek-Mercer (JM) smoothing:\n",
    "$$score(q,d) = \\sum_{t \\in q} log ((1-\\lambda) \\frac{c(t, d)}{|d|} + \\lambda \\frac{c (t, C)}{|C|}),$$\n",
    "where $c (t, d)$ and $c (t, C)$ represent frequency of a term in a document and collection, respectively.\n",
    "\n",
    "**Notes about your implementation:**\n",
    "- Skip a term if it does not exist in the whole collection. This avoids $log(0)$.\n",
    "- Make sure to use the right form of a query (analyzed vs. not analyzed)\n",
    "- Use natural logarithm \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Obtain collection length\n",
    "In this code, the global variable `len_C` denotes collection length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "global len_C\n",
    "# =======Your code=======\n",
    "\n",
    "# =======================\n",
    "\n",
    "print(\"Collection length =\", len_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Obtain document length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_doc(d):\n",
    "    # =======Your code=======\n",
    "\n",
    "    # =======================\n",
    "    return len_d\n",
    "\n",
    "doc = \"2674124\" # this is an example document\n",
    "print(\"Length of document \\\"\"+doc+\"\\\":\", len_doc(doc))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Obtain collection frequency of a term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coll_freq(t):\n",
    "    # =======Your code=======\n",
    "\n",
    "    # =======================\n",
    "    return cf\n",
    "term = \"record\"\n",
    "print(\"Frequency of term \\\"\"+term+\"\\\" in the collection:\", coll_freq(term)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Obtain term frequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(t, d):\n",
    "    # =======Your code=======\n",
    "\n",
    "    # =======================\n",
    "    return tf\n",
    "\n",
    "term = \"record\"\n",
    "doc = \"2674124\"\n",
    "print(\"Frequency of term \\\"\"+term+\"\\\" in document \\\"\"+doc+\"\\\":\", term_freq(term, doc)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Compute JM-smoothed probability for a single term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_t_Md(t, d, lambd):\n",
    "    # =======Your code=======\n",
    "\n",
    "    # =======================\n",
    "    return p_t_Md\n",
    "\n",
    "term = \"record\"\n",
    "doc = \"2674124\" \n",
    "print(\"p(t|Md) =\", prob_t_Md(term, doc, 0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Compute JM-smoothed probability for a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def score_doc(q, d, lambd):\n",
    "    # =======Your code=======\n",
    "\n",
    "    # =======================\n",
    "    return p_q_Md\n",
    "\n",
    "query = index_reader.analyze(queries[\"23849\"])\n",
    "doc = \"2674124\" \n",
    "print(\"p(q|Md) =\", score_doc(query, doc, 0.1))            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rank documents for the given queries\n",
    "Ranking is done in two steps:\n",
    "1. First pass retrieval: Use a fast ranker (i.e., Anserini SimpleSearcher) ro rank all documents for a given query.\n",
    "2. Second pass retrieval: Re-rank top-100 documents from the 1st pass retrieval using your retrieval model. This is to make the ranking process efficient.\n",
    "\n",
    "**Notes:**\n",
    "- You need to change the default values of SimpleSearcher functions to obtain top-100 documents\n",
    "- Set the value of lambda to 0.1\n",
    "- Store your final ranking results in the `results` variable. Every item in the `results` list is a list containing queryID, documentID, and score. This is an example how the content of results should look like:\n",
    "\n",
    "`[['23849', '4348282', -10.65],\n",
    " ['23849', '7119957', -12.63],\n",
    " ['23849', '', -17.687729001682484], \n",
    " ...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyserini.search import SimpleSearcher\n",
    "\n",
    "results = []\n",
    "searcher = SimpleSearcher(\"indexes/lucene-index-msmarco-passage\")\n",
    "# =======Your code=======\n",
    "\n",
    "# =======================\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your results into a file.\n",
    "Submit this file together with the completed notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check duplicates\n",
    "check = set()\n",
    "for res in results:\n",
    "    if ((res[0], res[1])) in check:\n",
    "        raise Exception(\"Error: Duplicate query-doc is found\", res[0], res[1])\n",
    "    check.add((res[0], res[1]))\n",
    "\n",
    "# write results in a file\n",
    "output_str = \"\\n\".join([l[0] + \"\\tQ0\\t\" + l[1] + \"\\t0\\t\" + str(l[2]) + \"\\tlm_jm\" for l in results])\n",
    "open(\"lm_jm.run\", \"w\").write(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handing in\n",
    "Submit the result file (ranked documents), the filled-in notebook, and the pdf version of your notebook:\n",
    "\n",
    "- The result file should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_lm_jm.run\n",
    "- The notebook should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_retrieval.ipynb\n",
    "- The pdf version of your notebook should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_retrieval.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
